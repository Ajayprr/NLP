{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation instructions\n",
    "https://textblob.readthedocs.io/en/dev/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: nltk>=3.1 in /anaconda3/envs/keras-tf/lib/python3.6/site-packages (from textblob) (3.3)\n",
      "Requirement not upgraded as not directly required: six in /anaconda3/envs/keras-tf/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the full corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "! python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or Download the mini corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/subashgandyer/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "! python -m textblob.download_corpora lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "#### Tokenization refers to splitting a large paragraph into sentences or words. Typically, a token refers to a word in a text document. Tokenization is pretty straight forward with TextBlob. All you have to do is import the TextBlob object from the textblob library, pass it the document that you want to tokenize, and then use the sentences and words attributes to get the tokenized sentences and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = (\"In computer science, artificial intelligence (AI), \\\n",
    "            sometimes called machine intelligence, is intelligence \\\n",
    "            demonstrated by machines, in contrast to the natural intelligence \\\n",
    "            displayed by humans and animals. Computer science defines AI \\\n",
    "            research as the study of \\\"intelligent agents\\\": any device that \\\n",
    "            perceives its environment and takes actions that maximize its\\\n",
    "            chance of successfully achieving its goals.[1] Colloquially,\\\n",
    "            the term \\\"artificial intelligence\\\" is used to describe machines\\\n",
    "            that mimic \\\"cognitive\\\" functions that humans associate with other\\\n",
    "            human minds, such as \\\"learning\\\" and \\\"problem solving\\\".[2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_blob_object = TextBlob(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"In computer science, artificial intelligence (AI),             sometimes called machine intelligence, is intelligence             demonstrated by machines, in contrast to the natural intelligence             displayed by humans and animals.\"), Sentence(\"Computer science defines AI             research as the study of \"intelligent agents\": any device that             perceives its environment and takes actions that maximize its            chance of successfully achieving its goals.\"), Sentence(\"[1] Colloquially,            the term \"artificial intelligence\" is used to describe machines            that mimic \"cognitive\" functions that humans associate with other            human minds, such as \"learning\" and \"problem solving\".\"), Sentence(\"[2]\")]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "document_sentence = text_blob_object.sentences\n",
    "\n",
    "print(document_sentence)\n",
    "print(len(document_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'computer', 'science', 'artificial', 'intelligence', 'AI', 'sometimes', 'called', 'machine', 'intelligence', 'is', 'intelligence', 'demonstrated', 'by', 'machines', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'displayed', 'by', 'humans', 'and', 'animals', 'Computer', 'science', 'defines', 'AI', 'research', 'as', 'the', 'study', 'of', 'intelligent', 'agents', 'any', 'device', 'that', 'perceives', 'its', 'environment', 'and', 'takes', 'actions', 'that', 'maximize', 'its', 'chance', 'of', 'successfully', 'achieving', 'its', 'goals', '1', 'Colloquially', 'the', 'term', 'artificial', 'intelligence', 'is', 'used', 'to', 'describe', 'machines', 'that', 'mimic', 'cognitive', 'functions', 'that', 'humans', 'associate', 'with', 'other', 'human', 'minds', 'such', 'as', 'learning', 'and', 'problem', 'solving', '2']\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "document_words = text_blob_object.words\n",
    "\n",
    "print(document_words)\n",
    "print(len(document_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "#### Lemmatization refers to reducing the word to its root form as found in a dictionary. To perform lemmatization via TextBlob, you have to use the Word object from the textblob library, pass it the word that you want to lemmatize and then call the lemmatize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apples: apple\n",
      "media: medium\n",
      "greater: great\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "word1 = Word(\"apples\")\n",
    "print(\"apples:\", word1.lemmatize())\n",
    "\n",
    "word2 = Word(\"media\")\n",
    "print(\"media:\", word2.lemmatize())\n",
    "\n",
    "word3 = Word(\"greater\")\n",
    "print(\"greater:\", word3.lemmatize(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts Of Speech (POS) Tagging\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In => IN\n",
      "computer => NN\n",
      "science => NN\n",
      "artificial => JJ\n",
      "intelligence => NN\n",
      "AI => NNP\n",
      "sometimes => RB\n",
      "called => VBD\n",
      "machine => NN\n",
      "intelligence => NN\n",
      "is => VBZ\n",
      "intelligence => NN\n",
      "demonstrated => VBN\n",
      "by => IN\n",
      "machines => NNS\n",
      "in => IN\n",
      "contrast => NN\n",
      "to => TO\n",
      "the => DT\n",
      "natural => JJ\n",
      "intelligence => NN\n",
      "displayed => VBN\n",
      "by => IN\n",
      "humans => NNS\n",
      "and => CC\n",
      "animals => NNS\n",
      "Computer => NNP\n",
      "science => NN\n",
      "defines => NNS\n",
      "AI => NNP\n",
      "research => NN\n",
      "as => IN\n",
      "the => DT\n",
      "study => NN\n",
      "of => IN\n",
      "intelligent => JJ\n",
      "agents => NNS\n",
      "any => DT\n",
      "device => NN\n",
      "that => WDT\n",
      "perceives => VBZ\n",
      "its => PRP$\n",
      "environment => NN\n",
      "and => CC\n",
      "takes => VBZ\n",
      "actions => NNS\n",
      "that => IN\n",
      "maximize => VB\n",
      "its => PRP$\n",
      "chance => NN\n",
      "of => IN\n",
      "successfully => RB\n",
      "achieving => VBG\n",
      "its => PRP$\n",
      "goals => NNS\n",
      "[ => RB\n",
      "1 => CD\n",
      "] => NNP\n",
      "Colloquially => NNP\n",
      "the => DT\n",
      "term => NN\n",
      "artificial => JJ\n",
      "intelligence => NN\n",
      "is => VBZ\n",
      "used => VBN\n",
      "to => TO\n",
      "describe => VB\n",
      "machines => NNS\n",
      "that => IN\n",
      "mimic => JJ\n",
      "cognitive => JJ\n",
      "functions => NNS\n",
      "that => WDT\n",
      "humans => NNS\n",
      "associate => VBP\n",
      "with => IN\n",
      "other => JJ\n",
      "human => JJ\n",
      "minds => NNS\n",
      "such => JJ\n",
      "as => IN\n",
      "learning => VBG\n",
      "and => CC\n",
      "problem => NN\n",
      "solving => NN\n",
      "[ => RB\n",
      "2 => CD\n",
      "] => NNS\n"
     ]
    }
   ],
   "source": [
    "for word, pos in text_blob_object.tags:\n",
    "    print(word + \" => \" + pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular, Plural\n",
    "#### TextBlob also allows you to convert text into a plural or singular form using the pluralize and singularize methods, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Footballs', 'iss', 'some', 'goods', 'games', 'Its', 'hass', 'manies', 'healths', 'benefits']\n"
     ]
    }
   ],
   "source": [
    "text = (\"Football is a good game. It has many health benefit\")\n",
    "text_blob_object = TextBlob(text)\n",
    "print(text_blob_object.words.pluralize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Football', 'is', 'a', 'good', 'game', 'It', 'ha', 'many', 'health', 'benefit']\n"
     ]
    }
   ],
   "source": [
    "text = (\"Footballs is a goods games. Its has many healths benefits\")\n",
    "\n",
    "text_blob_object = TextBlob(text)\n",
    "print(text_blob_object.words.singularize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun phrase extraction\n",
    "#### Noun phrase extraction refers to extracting phrases that contain nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer science\n",
      "artificial intelligence\n",
      "ai\n",
      "machine intelligence\n",
      "natural intelligence\n",
      "computer\n",
      "science defines\n",
      "ai\n",
      "intelligent agents\n",
      "colloquially\n",
      "artificial intelligence\n",
      "describe machines\n",
      "human minds\n"
     ]
    }
   ],
   "source": [
    "text_blob_object = TextBlob(document)\n",
    "for noun_phrase in text_blob_object.noun_phrases:\n",
    "    print(noun_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Words and Phrase Counts\n",
    "#### To find the frequency of occurrence of a particular word, we have to pass the name of the word as the index to the word_counts list of the TextBlob object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_blob_object = TextBlob(document)\n",
    "text_blob_object.word_counts['intelligence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_blob_object.words.count('intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_blob_object.words.count('intelligence', case_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_blob_object = TextBlob(document)\n",
    "text_blob_object.noun_phrases.count('artificial intelligence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Upper and Lowercase\n",
    "#### TextBlob objects are very similar to strings. You can convert them to upper case or lower case, change their values, and concatenate them together as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I LOVE TO WATCH FOOTBALL, BUT I HAVE NEVER PLAYED IT\n"
     ]
    }
   ],
   "source": [
    "text = \"I love to watch football, but I have never played it\"\n",
    "text_blob_object = TextBlob(text)\n",
    "\n",
    "print(text_blob_object.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love to watch football, but i have never played it\n"
     ]
    }
   ],
   "source": [
    "text = \"I LOVE TO WATCH FOOTBALL, BUT I HAVE NEVER PLAYED IT\"\n",
    "text_blob_object = TextBlob(text)\n",
    "\n",
    "print(text_blob_object.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding N-Grams\n",
    "#### N-Grams refer to n combination of words in a sentence. For instance, for a sentence \"I love watching football\", some 2-grams would be (I love), (love watching) and (watching football). N-Grams can play a crucial role in text classification.\n",
    "\n",
    "#### In TextBlob, N-grams can be found by passing the number of N-Grams to the ngrams method of the TextBlob object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love']\n",
      "['love', 'to']\n",
      "['to', 'watch']\n",
      "['watch', 'football']\n",
      "['football', 'but']\n",
      "['but', 'I']\n",
      "['I', 'have']\n",
      "['have', 'never']\n",
      "['never', 'played']\n",
      "['played', 'it']\n"
     ]
    }
   ],
   "source": [
    "text = \"I love to watch football, but I have never played it\"\n",
    "text_blob_object = TextBlob(text)\n",
    "for ngram in text_blob_object.ngrams(2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Corrections\n",
    "#### Spelling correction is one of the unique functionalities of the TextBlob library. With the correct method of the TextBlob object, you can correct all the spelling mistakes in your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to watch football, but I have never played it\n"
     ]
    }
   ],
   "source": [
    "text = \"I love to watchf footbal, but I have neter played it\"\n",
    "text_blob_object = TextBlob(text)\n",
    "\n",
    "print(text_blob_object.correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Translation\n",
    "#### One of the most powerful capabilities of the TextBlob library is to translate from one language to another. On the backend, the TextBlob language translator uses the Google Translate API\n",
    "\n",
    "#### To translate from one language to another, you simply have to pass the text to the TextBlob object and then call the translate method on the object. The language code for the language that you want your text to be translated to is passed as a parameter to the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "text_blob_object_french = TextBlob(u'Salut comment allez-vous?')\n",
    "print(text_blob_object_french.translate(to='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are you?\n"
     ]
    }
   ],
   "source": [
    "text_blob_object_arabic = TextBlob(u'مرحبا كيف حالك؟')\n",
    "print(text_blob_object_arabic.translate(to='en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n"
     ]
    }
   ],
   "source": [
    "text_blob_object = TextBlob(u'Hola como estas?')\n",
    "print(text_blob_object.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
